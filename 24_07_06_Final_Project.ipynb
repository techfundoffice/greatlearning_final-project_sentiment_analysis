{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techfundoffice/greatlearning_final-project_sentiment_analysis/blob/main/24_07_06_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-kfSgIGzgGL"
      },
      "source": [
        "# **Project : A Case Study of ExpressWay Logistics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Z-Vp4pzgGN"
      },
      "source": [
        "**Business Overview:**\n",
        "\n",
        "ExpressWay Logistics is a dynamic logistics service provider, committed to delivering efficient, reliable and cost-effective courier transportation and warehousing solutions. With a focus on speed, precision and customer satisfaction, we aim to be the go-to partner for our customers seeking seamless courier services. Our core service involves ensuring operational efficiency throughout our delivery and courier services, including inventory management, durable packaging and swift dispatch of couriers, real time tracking of shipments and on-time delivery of couriers as promised. We are committed to enhance our logistics and courier services and improve seamless connectivity for our customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46XFRcFTzgGN"
      },
      "source": [
        "**Current Challenge:**\n",
        "\n",
        "ExpressWay Logistics faces numerous challenges in ensuring seamless deliveries and customer satisfaction. These challenges include managing various customer demands simultaneously, addressing delays in deliveries and ensuring products arrive intact and safe. Additionally, the company struggles with complexity of efficiently storing and handling a large volume of packages and ultimately meeting customer expectations. Moreover, maintaining a skilled workforce capable of handling various aspects of logistics operations presents its own set of challenges. Overcoming these obstacles requires a comprehensive approach that integrates innovative technology, strategic planning, and continuous improvement initiatives to ensure smooth operations and exceptional service delivery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maC1EgUJzgGO"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "Our primary objective is to conduct a sentiment analysis of user-generated reviews across various digital channels and platforms. By paying attention to their feedback, we want to find ways to make our services better - like handling different customer demands simultaneously, dealing with late deliveries, and keeping packages secured and intact. Through the application of prompt engineering methodologies and sentiment analysis, we'll figure out if sentiments expressed by users for our courier services are Positive or Negative. This will help us understand where we need to improve in order to meet customer expectations and keep them happy. With a focus on getting better all the time, we'll overcome the challenges at ExpressWay Logistics and make our services the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUHF3xczgGO"
      },
      "source": [
        "**Data Description:**\n",
        "\n",
        "The dataset titled \"courier-service_reviews.csv\" is structured to facilitate sentiment analysis for courier service reviews. Here's a brief description of the data columns:\n",
        "\n",
        "1. id: This column contains unique identifiers for each review entry. It helps in distinguishing and referencing individual reviews.\n",
        "2. review: This column includes the actual text of the courier service reviews. The reviews are likely composed of customer opinions and experiences regarding different aspects of the services provided by ExpressWay Logistics.\n",
        "3. sentiment: This column provides an additional layer of classification (positive and negative) for the mentioned reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zTC8qPRPahk"
      },
      "source": [
        "##**Step 1. Setup (2 Marks)**\n",
        "\n",
        "(A) Writing/Creating the config.json file  (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the configuration settings\n",
        "config = {\n",
        "    \"openai_api_key\": \"your_openai_api_key\",\n",
        "    \"dataset_path\": \"/content/courier-service_reviews.csv\",\n",
        "    \"model\": {\n",
        "        \"name\": \"gpt-3.5-turbo\",\n",
        "        \"temperature\": 0,\n",
        "        \"max_tokens\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save the configuration to a JSON file\n",
        "with open('config.json', 'w') as config_file:\n",
        "    json.dump(config, config_file, indent=4)\n",
        "\n",
        "print(\"config.json file has been created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2kPlF2d3p_3",
        "outputId": "d13cb030-b938-4914-f18b-d305e7fc6879"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json file has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLYC3zzSr8z"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "keb94_PFzgGO"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.2 tiktoken datasets session-info --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFZj9kr8Pahl"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8VP8o8hyQ4uw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q3gwxSqQPahl"
      },
      "outputs": [],
      "source": [
        "# Import all Python packages required to access the Azure Open AI API.\n",
        "# Import additional packages required to access datasets and create examples.\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import random\n",
        "import tiktoken\n",
        "import session_info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6mmXtfmrS7Ml",
        "outputId": "b98f3bc4-280e-48c4-965d-4621412388d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<details>\n",
              "<summary>Click to view session information</summary>\n",
              "<pre>\n",
              "-----\n",
              "numpy               1.25.2\n",
              "openai              1.2.0\n",
              "pandas              2.0.3\n",
              "session_info        1.0.0\n",
              "sklearn             1.2.2\n",
              "tabulate            0.9.0\n",
              "tiktoken            NA\n",
              "tqdm                4.66.4\n",
              "-----\n",
              "</pre>\n",
              "<details>\n",
              "<summary>Click to view modules imported as dependencies</summary>\n",
              "<pre>\n",
              "Cython              3.0.10\n",
              "PIL                 9.4.0\n",
              "annotated_types     0.7.0\n",
              "anyio               NA\n",
              "attr                23.2.0\n",
              "backcall            0.2.0\n",
              "certifi             2024.06.02\n",
              "cffi                1.16.0\n",
              "click               8.1.7\n",
              "cloudpickle         2.2.1\n",
              "cycler              0.12.1\n",
              "cython              3.0.10\n",
              "cython_runtime      NA\n",
              "dateutil            2.8.2\n",
              "debugpy             1.6.6\n",
              "decorator           4.4.2\n",
              "defusedxml          0.7.1\n",
              "distro              1.7.0\n",
              "entrypoints         0.4\n",
              "google              NA\n",
              "h11                 0.14.0\n",
              "httpcore            1.0.5\n",
              "httplib2            0.22.0\n",
              "httpx               0.27.0\n",
              "idna                3.7\n",
              "ipykernel           5.5.6\n",
              "ipyparallel         8.8.0\n",
              "ipython_genutils    0.2.0\n",
              "joblib              1.4.2\n",
              "kiwisolver          1.4.5\n",
              "matplotlib          3.7.1\n",
              "matplotlib_inline   0.1.7\n",
              "mpl_toolkits        NA\n",
              "numexpr             2.10.1\n",
              "packaging           24.1\n",
              "pexpect             4.9.0\n",
              "pickleshare         0.7.5\n",
              "pkg_resources       NA\n",
              "platformdirs        4.2.2\n",
              "portpicker          NA\n",
              "prompt_toolkit      3.0.47\n",
              "psutil              5.9.5\n",
              "ptyprocess          0.7.0\n",
              "pyarrow             16.1.0\n",
              "pydantic            2.8.0\n",
              "pydantic_core       2.20.0\n",
              "pydev_ipython       NA\n",
              "pydevconsole        NA\n",
              "pydevd              2.9.5\n",
              "pydevd_file_utils   NA\n",
              "pydevd_plugins      NA\n",
              "pydevd_tracing      NA\n",
              "pygments            2.16.1\n",
              "pyparsing           3.1.2\n",
              "pytz                2023.4\n",
              "regex               2.5.145\n",
              "rich                NA\n",
              "scipy               1.11.4\n",
              "setuptools          67.7.2\n",
              "sitecustomize       NA\n",
              "six                 1.16.0\n",
              "sniffio             1.3.1\n",
              "socks               1.7.1\n",
              "sphinxcontrib       NA\n",
              "storemagic          NA\n",
              "threadpoolctl       3.5.0\n",
              "tiktoken_ext        NA\n",
              "tornado             6.3.3\n",
              "traitlets           5.7.1\n",
              "typing_extensions   NA\n",
              "wcwidth             0.2.13\n",
              "zmq                 24.0.1\n",
              "zoneinfo            NA\n",
              "</pre>\n",
              "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
              "<pre>\n",
              "-----\n",
              "IPython             7.34.0\n",
              "jupyter_client      6.1.12\n",
              "jupyter_core        5.7.2\n",
              "notebook            6.5.5\n",
              "-----\n",
              "Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
              "Linux-6.1.85+-x86_64-with-glibc2.35\n",
              "-----\n",
              "Session information updated at 2024-07-08 01:35\n",
              "</pre>\n",
              "</details>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "session_info.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN0wIDUjPahn"
      },
      "source": [
        "### Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvDZdCMhzgGQ"
      },
      "source": [
        "**(A) Writing/Creating the config.json file (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2tdRk05EzgGQ"
      },
      "outputs": [],
      "source": [
        "# Define your configuration information\n",
        "config_data = {\n",
        "    \"AZURE_OPENAI_KEY\": \"4c37b0d693d74236b3cc8a77bd14f36a\",\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"https://sentimentanalysisfinal.openai.azure.com/\",\n",
        "    \"AZURE_OPENAI_APIVERSION\": \"2024-02-01\",\n",
        "    \"CHATGPT_MODEL\": \"gpt-3.5-turbo\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO4TwVYHzgGQ",
        "outputId": "a395ee8b-08ff-4969-fd3b-950f27614464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config file created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Write the configuration information into the config.json file\n",
        "with open('config.json', 'w') as config_file:\n",
        "    json.dump(config_data, config_file, indent=4)\n",
        "\n",
        "print(\"Config file created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "reviews_df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame to confirm successful read\n",
        "print(\"Successfully read the CSV file from the specified path.\")\n",
        "reviews_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Gu9HprYkM8DL",
        "outputId": "7be424e8-377f-49f6-9199-75c1258052bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read the CSV file from the specified path.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                             review sentiment\n",
              "0   1  ExpressWay Logistics' commitment to transparen...  Positive\n",
              "1   2  The tracking system implemented by ExpressWay ...  Positive\n",
              "2   3  ExpressWay Logistics is a lifesaver when it co...  Positive\n",
              "3   4  Expressway Logistics is the worst courier serv...  Negative\n",
              "4   5  ExpressWay Logistics failed to meet my expecta...  Negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7fd7c6e-8fdd-4e13-9110-ed20d517dd9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>ExpressWay Logistics' commitment to transparen...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The tracking system implemented by ExpressWay ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ExpressWay Logistics is a lifesaver when it co...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Expressway Logistics is the worst courier serv...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>ExpressWay Logistics failed to meet my expecta...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7fd7c6e-8fdd-4e13-9110-ed20d517dd9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7fd7c6e-8fdd-4e13-9110-ed20d517dd9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7fd7c6e-8fdd-4e13-9110-ed20d517dd9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08a1a59d-d41d-4660-b94c-65117617b8ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08a1a59d-d41d-4660-b94c-65117617b8ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08a1a59d-d41d-4660-b94c-65117617b8ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "reviews_df",
              "summary": "{\n  \"name\": \"reviews_df\",\n  \"rows\": 131,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37,\n        \"min\": 1,\n        \"max\": 131,\n        \"num_unique_values\": 131,\n        \"samples\": [\n          56,\n          41,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 131,\n        \"samples\": [\n          \"ExpressWay Logistics offers a convenient online platform for booking shipments, and their competitive rates initially drew me in. However, the frequent delays in delivery and lack of transparency regarding additional fees have soured my experience. While their customer service representatives are polite, they often seem powerless to resolve issues effectively.This lack of care and responsibilites from the concerned teams have led me unsatisfied and irritated.I cannot trust and rely on this again.Will definitely not contact them ever for my courier deliveries!\",\n          \"ExpressWay Logistics' pricing may seem attractive at first glance, but beware of some internal hidden fees that may sometimes add up. Coupled with their reliable delivery times, it's simply not worth the hassle.But still I appreciate that my parcel got delivered safely a day after the promised window.\",\n          \"I was deeply disappointed by the behavior of the delivery executive from Expressway Logistics. They exhibited a complete disregard for customer satisfaction, arriving late and mishandling the package upon delivery. Despite raising my concerns with Expressway Logistics, there was no acknowledgment or resolution provided.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ugdC-geqzTZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(B) Count Positive and Negative Sentiment Reviews (1 Marks)**"
      ],
      "metadata": {
        "id": "eazomNhVzUws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using dataframe reviews_df: (B) Count Positive and Negative Sentiment Reviews (1\n",
        "\n",
        "reviews_df['sentiment'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEzAVsrUQpaA",
        "outputId": "e047565d-d003-4eb8-ed18-2f21cb960ddb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "Positive    68\n",
              "Negative    63\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (C) Split the Dataset (2 Marks)"
      ],
      "metadata": {
        "id": "UeWo0U8G0LdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets (80-20 split)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the distribution of sentiments in the training and testing sets\n",
        "train_distribution = train_df['sentiment'].value_counts()\n",
        "test_distribution = test_df['sentiment'].value_counts()\n",
        "\n",
        "train_distribution, test_distribution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uvQq-KtYFsz",
        "outputId": "6e823acd-64e1-4825-c8f4-8fb14d88d872"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(sentiment\n",
              " Positive    54\n",
              " Negative    50\n",
              " Name: count, dtype: int64,\n",
              " sentiment\n",
              " Positive    14\n",
              " Negative    13\n",
              " Name: count, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cxLhZIviPahn"
      },
      "outputs": [],
      "source": [
        "with open('config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nL9GpBEVPahn"
      },
      "outputs": [],
      "source": [
        "creds = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ud7bPDquPahp"
      },
      "outputs": [],
      "source": [
        "client = AzureOpenAI(\n",
        "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
        "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Atf3l2mNPahp"
      },
      "outputs": [],
      "source": [
        "chat_model_id = creds[\"CHATGPT_MODEL\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQpkd5elTMhd"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V5Gp-_CxPahp"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the number of tokens used by a list of messages.\n",
        "    Adapted from the Open AI cookbook token counter\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Each message is sandwiched with <|start|>role and <|end|>\n",
        "    # Hence, messages look like: <|start|>system or user or assistant{message}<|end|>\n",
        "\n",
        "    tokens_per_message = 3 # token1:<|start|>, token2:system(or user or assistant), token3:<|end|>\n",
        "\n",
        "    num_tokens = 0\n",
        "\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_messages(messages):\n",
        "    \"\"\"\n",
        "    Return the number of tokens used by a list of messages.\n",
        "    Adapted from the Open AI cookbook token counter\n",
        "    \"\"\"\n",
        "    print(\"Return the number of tokens used by a list of messages. Adapted from the Open AI cookbook token counter\")\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Each message is sandwiched with role and content tokens\n",
        "    # Hence, messages look like: system or user or assistant{message}\n",
        "\n",
        "    tokens_per_message = 3  # token1: role, token2: content, token3: message delimiter\n",
        "    tokens_per_name = 1  # if there is a name field, it adds an extra token\n",
        "\n",
        "    num_tokens = 0\n",
        "\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == 'name':\n",
        "                num_tokens += tokens_per_name\n",
        "\n",
        "    num_tokens += 3  # every reply is primed with assistant\n",
        "\n",
        "    return num_tokens\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"}\n",
        "]\n",
        "\n",
        "print(num_tokens_from_messages(messages))  # This will print the number of tokens used by the messages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnUC7Xsi5nO8",
        "outputId": "27915dbe-acdb-4d53-cffd-68de7fbcfbb3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return the number of tokens used by a list of messages. Adapted from the Open AI cookbook token counter\n",
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sP7s5TDPahq"
      },
      "source": [
        "## Task : Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhctrBwFPahq"
      },
      "source": [
        "##**Step 2: Assemble Data (5 Marks)**\n",
        "\n",
        "(A) Upload and Read csv File (2 Marks)\n",
        "\n",
        "(B) Count Positive and Negative Sentiment Reviews (1 Marks)\n",
        "\n",
        "(C) Split the Dataset (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLK-296WzgGS"
      },
      "source": [
        "**(A) Upload and read csv file (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SA5Cyqgk5kgB"
      },
      "outputs": [],
      "source": [
        " cs_reviews_df = \"/content/courier-service_reviews.csv\"\n",
        "# Read CSV File Here"
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file using the provided path\n",
        "cs_reviews_df = pd.read_csv('/content/courier-service_reviews.csv')\n",
        "\n",
        "# Now you can call the info() method\n",
        "cs_reviews_df.info()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3f0oKMvgPY",
        "outputId": "e24e6864-5aba-4524-cae9-7657ff82da3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 131 entries, 0 to 130\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         131 non-null    int64 \n",
            " 1   review     131 non-null    object\n",
            " 2   sentiment  131 non-null    object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UJmr1iJ5_tNm",
        "outputId": "96e971b5-3c3d-44d9-abc6-ca4b3fc37f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id                                             review sentiment\n",
              "107  108  ExpressWay Logistics' commitment to customer s...  Positive\n",
              "63    64  ExpressWay Logistics promises efficient delive...  Negative\n",
              "77    78  ExpressWay Logistics is unreliable and untrust...  Negative\n",
              "55    56  ExpressWay Logistics offers a convenient onlin...  Negative\n",
              "85    86  ExpressWay Logistics' user-friendly online pla...  Negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae1b65a4-41b1-438d-9592-5d685609a5fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>108</td>\n",
              "      <td>ExpressWay Logistics' commitment to customer s...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>64</td>\n",
              "      <td>ExpressWay Logistics promises efficient delive...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>78</td>\n",
              "      <td>ExpressWay Logistics is unreliable and untrust...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>56</td>\n",
              "      <td>ExpressWay Logistics offers a convenient onlin...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>86</td>\n",
              "      <td>ExpressWay Logistics' user-friendly online pla...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1b65a4-41b1-438d-9592-5d685609a5fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae1b65a4-41b1-438d-9592-5d685609a5fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae1b65a4-41b1-438d-9592-5d685609a5fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcb1afbf-ccba-4604-9fbf-824d0acb7d9f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcb1afbf-ccba-4604-9fbf-824d0acb7d9f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcb1afbf-ccba-4604-9fbf-824d0acb7d9f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"cs_reviews_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 56,\n        \"max\": 108,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          64,\n          86,\n          78\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ExpressWay Logistics promises efficient delivery, but their drivers seem careless and unprofessional. I've received damaged goods more than once, indicating a lack of proper handling procedures.\",\n          \"ExpressWay Logistics' user-friendly online platform made it easy for me to schedule and track my shipment. The intuitive interface and clear instructions simplified the shipping process and provided a seamless experience.However, despite the convenience of the online platform, I encountered issues with the accuracy of the tracking information. The updates were delayed, leading to uncertainty about the status of my package and causing unnecessary anxiety.\",\n          \"ExpressWay Logistics is unreliable and untrustworthy. They failed to deliver my parcel on time, and the customer support team was unapologetic and unwilling to assist me in resolving the issue.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Negative\",\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "cs_reviews_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(B) Count Positive and Negative Sentiment Reviews (1 Marks)**"
      ],
      "metadata": {
        "id": "CKUDLCbC0bfB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5AbhvRV9o-e",
        "outputId": "217a6f6e-1484-435d-b6fc-7c8f8db32f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive reviews: 0\n",
            "Number of negative reviews: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "reviews_df = pd.read_csv(file_path)\n",
        "\n",
        "# Count the positive and negative reviews\n",
        "positive_count = reviews_df[reviews_df['sentiment'] == 'positive'].shape[0]\n",
        "negative_count = reviews_df[reviews_df['sentiment'] == 'negative'].shape[0]\n",
        "\n",
        "# Display the counts\n",
        "print(f\"Number of positive reviews: {positive_count}\")\n",
        "print(f\"Number of negative reviews: {negative_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Z0q0b6zgGT",
        "outputId": "59f55483-0603-4703-ef84-e06fdf6e5e7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "cs_reviews_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiXuxmchzgGT"
      },
      "source": [
        "**(C) Split the Dataset (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dkXtXbc9N5rC"
      },
      "outputs": [],
      "source": [
        "cs_examples_df, cs_gold_examples_df = train_test_split(\n",
        "    df,               # <- the full dataset\n",
        "    test_size=0.2,    # <- 20% random sample selected for gold examples\n",
        "    random_state=42   # <- ensures that the splits are the same for every session\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGSj2dfZOMW-",
        "outputId": "a0c2c871-a326-4a75-8cca-a3647461df50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((104, 3), (27, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "(cs_examples_df.shape, cs_gold_examples_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ExVGFPl4tdx"
      },
      "source": [
        "To select gold examples for this session, we sample randomly from the test data using a `random_state=42`. This ensures that the examples from multiple runs of the sampling are the same (i.e., they are randomly selected but do not change between different runs of the notebook). Note that we are doing this only to keep execution times low for illustration. In practise, large number of gold examples facilitate robust estimates of model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YLj4LVu_Ozc_"
      },
      "outputs": [],
      "source": [
        "columns_to_select = ['review','sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nDIZm2RdMsDt"
      },
      "outputs": [],
      "source": [
        "gold_examples = (\n",
        "        cs_gold_examples_df.loc[:, columns_to_select]\n",
        "                                     .sample(21, random_state=42) #<- ensures that gold examples are the same for every session\n",
        "                                     .to_json(orient='records')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "5AcH3eqgzgGV",
        "outputId": "a4779cff-13a7-46fc-f72a-e43cd19b10b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[{\"review\":\"The delivery executive assigned by ExpressWay Logistics was courteous and professional during the delivery process. They tried their best to handle the package with care.Unfortunately, the package arrived with slight damage despite the delivery executive\\'s efforts. The packaging seemed more than adequate to protect the contents during transit.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics failed to meet my expectations. The delivery was delayed, and the customer support team was unresponsive and unhelpful when I tried to inquire about the status of my parcel.\",\"sentiment\":\"Negative\"},{\"review\":\"ExpressWay Logistics\\' incompetence resulted in a major inconvenience when my package was delivered to the wrong recipient. Despite providing accurate delivery information, the package ended up in the hands of someone else, and efforts to retrieve it were unsuccessful. When I contacted customer service for assistance, I was met with apathy and a lack of urgency. Their failure to rectify the situation in a timely manner is unacceptable, and I will be avoiding their services at all costs.\",\"sentiment\":\"Negative\"},{\"review\":\"Kudos to ExpressWay Logistics for their outstanding service! They\\'ve been an essential partner for my business, handling all our shipping needs with professionalism and efficiency. If you\\'re looking for a reliable logistic service provider, look no further than ExpressWay Logistics.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics offers a convenient online platform for booking shipments, and their competitive rates initially drew me in. However, the frequent delays in delivery and lack of transparency regarding additional fees have soured my experience. While their customer service representatives are polite, they often seem powerless to resolve issues effectively.This lack of care and responsibilites from the concerned teams have led me unsatisfied and irritated.I cannot trust and rely on this again.Will definitely not contact them ever for my courier deliveries!\",\"sentiment\":\"Negative\"},{\"review\":\"Planning a destination wedding is undoubtedly a daunting task, especially when it comes to coordinating transportation for guests and wedding essentials. Fortunately, ExpressWay Logistics made this aspect of our wedding planning effortless. From the beginning, they were attentive to our needs and provided valuable insights to streamline the process.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics consistently delivers on its promises, providing fast, efficient, and reliable courier services that exceeded my expectations.\",\"sentiment\":\"Positive\"},{\"review\":\"I needed to ship perishable items internationally, and ExpressWay Logistics handled the process flawlessly. They ensured that the items were properly packaged and transported in temperature-controlled conditions.\",\"sentiment\":\"Positive\"},{\"review\":\"I appreciated the flexibility offered by ExpressWay Logistics in terms of delivery options. Their range of delivery speeds and service levels allowed me to choose the option that best suited my needs and budget.However, I was slightly disappointed by the lack of transparency regarding few additional fees and surcharges. I selected a specific delivery option and paid minor charges at checkout, which undermined the initial affordability of the service..\",\"sentiment\":\"Negative\"},{\"review\":\"While ExpressWay Logistics\\' pricing is competitive, their lack of accountability for delays and damages is disappointing. It\\'s frustrating to pay for a service that fails to deliver on its promises consistently.\",\"sentiment\":\"Negative\"},{\"review\":\"ExpressWay Logistics\\' pricing may seem attractive at first glance, but beware of some internal hidden fees that may sometimes add up. Coupled with their reliable delivery times, it\\'s simply not worth the hassle.But still I appreciate that my parcel got delivered safely a day after the promised window.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics has been my go-to courier service for years, primarily due to their extensive network coverage and competitive pricing. However, my recent experience has left me questioning their reliability. While their user-friendly website and efficient booking process remain commendable, the repeated delays in delivery and lack of proactive communication have been disappointing.Despite these setbacks, their customer service team has always been courteous and responsive, attempting to address my concerns promptly.\",\"sentiment\":\"Negative\"},{\"review\":\"ExpressWay Logistics\\' incompetence led to the loss of my parcel, and their lack of accountability only added to the frustration. Despite filing a claim and providing evidence of the missing items, I received no assistance or compensation for the lost package. Their negligence and disregard for customer satisfaction are unacceptable, and I will be exploring legal options to recoup the value of the lost items.\",\"sentiment\":\"Negative\"},{\"review\":\"I was deeply disappointed by the behavior of the delivery executive from Expressway Logistics. They exhibited a complete disregard for customer satisfaction, arriving late and mishandling the package upon delivery. Despite raising my concerns with Expressway Logistics, there was no acknowledgment or resolution provided.\",\"sentiment\":\"Negative\"},{\"review\":\"As a frequent traveler, I often use ExpressWay Logistics to ship luggage and personal belongings ahead of time. Their door-to-door service and reliable tracking make traveling a breeze.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics caught my attention with its promise of seamless shipping solutions and a comprehensive network. However, my recent experiences have revealed some significant shortcomings. Their online platform is disgusting and their tracking system is inaccurate.I\\'ve encountered repeated issues with inaccurate weight calculations that have led to unexpected charges. Moreover, the limited options for customizing shipment preferences have made it challenging to meet my requirements.I cannot rely upon it for my future requirements.It failed to meet my requirements.\",\"sentiment\":\"Negative\"},{\"review\":\"The packaging provided by ExpressWay Logistics is always secure and well-protected, and ensured that my items arrive in perfect condition.\",\"sentiment\":\"Positive\"},{\"review\":\"My recent experience with ExpressWay Logistics left me thoroughly impressed. I had a highly time-sensitive shipment that required delicate handling and precise coordination. Despite the challenges, ExpressWay rose to the occasion, demonstrating an unparalleled level of professionalism and efficiency. Their team meticulously planned every aspect of the shipment, from packaging to transportation, ensuring that no detail was overlooked. \",\"sentiment\":\"Positive\"},{\"review\":\"I was initially impressed by ExpressWay Logistics\\' promise of reliability and affordability, but my recent encounters have been less than satisfactory. Their website is so irritating and their drivers are too harsh, the repeated delays in delivery have been frustrating. Additionally, the discovery of hidden fees within their pricing structure has left me feeling misled. In addition to these challenges, their customer service team has been inattentive and disrespectful, which has degraded the overall experience.Not at all recommend! \",\"sentiment\":\"Negative\"},{\"review\":\"Thumbs up to ExpressWay for their prompt responses and reliable shipping solutions. Impressed!\",\"sentiment\":\"Positive\"},{\"review\":\"I had a terrible experience with ExpressWay Logistics. Not only was my pickup delayed, but the packaging was not done intactly, and the customer support team was unresponsive when I tried to raise the issue.\",\"sentiment\":\"Negative\"}]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "gold_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8J58KMMM95s",
        "outputId": "1e15e69f-3534-4421-bf3d-4cfdd11aa8da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'review': \"The delivery executive assigned by ExpressWay Logistics was courteous and professional during the delivery process. They tried their best to handle the package with care.Unfortunately, the package arrived with slight damage despite the delivery executive's efforts. The packaging seemed more than adequate to protect the contents during transit.\",\n",
              " 'sentiment': 'Positive'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "json.loads(gold_examples)[0]     #Json format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPsU-h8FPaht"
      },
      "source": [
        "##**Step 3: Derive Prompt (12 Marks)**\n",
        "\n",
        "(A) Write Zero Shot System Message (3 Marks)\n",
        "\n",
        "(B) Create Zero Shot Prompt (2 Marks)\n",
        "\n",
        "(C) Write Few Shot System Message (3 Marks)\n",
        "\n",
        "(D) Create Examples For Few shot prompte (2 Marks)\n",
        "\n",
        "(E) Create Few Shot Prompt (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NfDUKbCgPahu"
      },
      "outputs": [],
      "source": [
        "user_message_template = \"\"\"```{courier_service_review}```\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3uS8NWfNqBG"
      },
      "source": [
        "**(A) Write Zero Shot System Message (3 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Zero Shot System Message\n",
        "zero_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\"\"\"\n",
        "\n",
        "# Display the Zero Shot System Message\n",
        "print(\"Zero Shot System Message:\")\n",
        "print(zero_shot_system_message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu6RFOqjcvT3",
        "outputId": "61e88992-a1f6-4d92-8a8c-b0acf063db71"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero Shot System Message:\n",
            "\n",
            "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46AWalvrLTxs"
      },
      "source": [
        "**(B) Create Zero Shot Prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "t8cjNjiJJpzc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18dba6fe-8fac-4dc7-ebfb-9207335eb11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero Shot Prompt:\n",
            "\n",
            "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
            "\n",
            "Review: The delivery was quick and the package arrived in perfect condition.\n",
            "Sentiment:\n"
          ]
        }
      ],
      "source": [
        "# Example review for the zero-shot prompt\n",
        "example_review = \"The delivery was quick and the package arrived in perfect condition.\"\n",
        "\n",
        "# Create zero shot prompt to be input-ready for the completion function\n",
        "zero_shot_prompt = zero_shot_system_message + \"\\nReview: \" + example_review + \"\\nSentiment:\"\n",
        "\n",
        "# Display the Zero Shot Prompt\n",
        "print(\"Zero Shot Prompt:\")\n",
        "print(zero_shot_prompt)\n"
      ]
    },
    {
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  if model == \"gpt-3.5-turbo\":\n",
        "      num_tokens = 0\n",
        "      for message in messages:\n",
        "          num_tokens += 4  # every message follows <im_start>{role/name}\\"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mlT_nL2fiM-G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(C) Write Few Shot System Message (3 Marks)**"
      ],
      "metadata": {
        "id": "48bF42gAyQ7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Few Shot System Message\n",
        "few_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\n",
        "Here are a few examples:\n",
        "\n",
        "Review: The delivery was quick and the package arrived in perfect condition.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: The package arrived late and the box was damaged.\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: Excellent service! The courier was very professional.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: Terrible experience. I will not use this service again.\n",
        "Sentiment: Negative\n",
        "\"\"\"\n",
        "\n",
        "# Display the Few Shot System Message\n",
        "print(\"Few Shot System Message:\")\n",
        "print(few_shot_system_message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMixUOG-fdI_",
        "outputId": "d76e3ee9-6003-4c1f-f0d8-53b3a95e2e0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few Shot System Message:\n",
            "\n",
            "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
            "\n",
            "Here are a few examples:\n",
            "\n",
            "Review: The delivery was quick and the package arrived in perfect condition.\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: The package arrived late and the box was damaged.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: Excellent service! The courier was very professional.\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: Terrible experience. I will not use this service again.\n",
            "Sentiment: Negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3ExNtgPRDhJ"
      },
      "source": [
        "Merely selecting random samples from the polarity subsets is not enough because the examples included in a prompt are prone to a set of known biases such as:\n",
        " - Majority label bias (frequent answers in predictions)\n",
        " - Recency bias (examples near the end of the prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEA_Ab2FRrYB"
      },
      "source": [
        "To avoid these biases, it is important to have a balanced set of examples that are arranged in random order. Let us create a Python function that generates bias-free examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qb0mLtE9R7ZA"
      },
      "outputs": [],
      "source": [
        "def create_examples(dataset, n=4):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a JSON list of randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (review + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "\n",
        "    Output:\n",
        "        randomized_examples (JSON): A JSON with examples in random order\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n)\n",
        "\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "\n",
        "    # sampling without replacement is equivalent to random shuffling\n",
        "\n",
        "    randomized_examples = examples.sample(2*n, replace=False)\n",
        "\n",
        "    return randomized_examples.to_json(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(D) Create Examples For Few shot prompte (2 Marks)**"
      ],
      "metadata": {
        "id": "vHhPExVTyO93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jb1VpSgZPaht"
      },
      "outputs": [],
      "source": [
        "examples = \"__________\"\n",
        "# Create Examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step (D) Create Examples For Few shot prompte (2 Marks)\n",
        "\n",
        "# Define the create_examples function\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def create_examples(dataset, n=4):\n",
        "    \"\"\"\n",
        "    Return a JSON list of randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (review + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "\n",
        "    Output:\n",
        "        randomized_examples (JSON): A JSON with examples in random order\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n, random_state=None)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n, random_state=None)\n",
        "\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "\n",
        "    # Randomize the order of the examples\n",
        "    randomized_examples = examples.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return randomized_examples.to_json(orient='records')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Create bias-free examples\n",
        "examples = create_examples(df, n=4)\n",
        "\n",
        "# Display the generated examples\n",
        "print(examples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59cHT39Ei_yL",
        "outputId": "2c16146d-09d0-4e6c-facb-9d7b81e5a225"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"review\":\"ExpressWay Logistics' dedication to safety is commendable. They prioritize employee training and adhere to strict safety protocols to ensure the secure handling and transport of all shipments.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics' team of dedicated customer service representatives is always available to assist us with any questions or concerns. They are knowledgeable, friendly, and responsive, ensuring that our shipping needs are met promptly and efficiently. With ExpressWay Logistics, we know that we are in good hands.\",\"sentiment\":\"Positive\"},{\"review\":\"Moving across the country is undoubtedly a stressful endeavor, and the logistics of it can often be overwhelming. However, with ExpressWay Logistics, the process was surprisingly smooth and stress-free. From the initial inquiry to the final delivery, their team provided unparalleled support and guidance. They meticulously planned every aspect of the move, from packing fragile items to coordinating the transportation of large furniture.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics' customer service representatives are polite but lack the authority to resolve issues effectively. Dealing with them feels like hitting a dead end, with problems often left unresolved.\",\"sentiment\":\"Negative\"},{\"review\":\"I had a terrible experience with ExpressWay Logistics. Not only was my parcel delayed, but the customer support team was rude and unresponsive when I tried to inquire about its whereabouts and I ended up with no resolution.\",\"sentiment\":\"Negative\"},{\"review\":\"I am extremely disappointed with the service provided by ExpressWay Logistics. My parcel was delivered late, and the packaging was damaged, resulting in the loss of valuable items.\",\"sentiment\":\"Negative\"},{\"review\":\"ExpressWay Logistics makes shipping hassle-free.Their tracking system is top-notch, giving me peace of mind every step of the way.I highly recommend ExpressWay Logistics for all your shipping needs.\",\"sentiment\":\"Positive\"},{\"review\":\"ExpressWay Logistics is unreliable and untrustworthy. They failed to deliver my parcel on time, and the customer support team was unapologetic and unwilling to assist me in resolving the issue.\",\"sentiment\":\"Negative\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJInVW2PSucc",
        "outputId": "4a2b28d7-76c9-4bd9-da26-90fc86da5e18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'review': \"ExpressWay Logistics' dedication to safety is commendable. They prioritize employee training and adhere to strict safety protocols to ensure the secure handling and transport of all shipments.\",\n",
              "  'sentiment': 'Positive'},\n",
              " {'review': \"ExpressWay Logistics' team of dedicated customer service representatives is always available to assist us with any questions or concerns. They are knowledgeable, friendly, and responsive, ensuring that our shipping needs are met promptly and efficiently. With ExpressWay Logistics, we know that we are in good hands.\",\n",
              "  'sentiment': 'Positive'},\n",
              " {'review': 'Moving across the country is undoubtedly a stressful endeavor, and the logistics of it can often be overwhelming. However, with ExpressWay Logistics, the process was surprisingly smooth and stress-free. From the initial inquiry to the final delivery, their team provided unparalleled support and guidance. They meticulously planned every aspect of the move, from packing fragile items to coordinating the transportation of large furniture.',\n",
              "  'sentiment': 'Positive'},\n",
              " {'review': \"ExpressWay Logistics' customer service representatives are polite but lack the authority to resolve issues effectively. Dealing with them feels like hitting a dead end, with problems often left unresolved.\",\n",
              "  'sentiment': 'Negative'},\n",
              " {'review': 'I had a terrible experience with ExpressWay Logistics. Not only was my parcel delayed, but the customer support team was rude and unresponsive when I tried to inquire about its whereabouts and I ended up with no resolution.',\n",
              "  'sentiment': 'Negative'},\n",
              " {'review': 'I am extremely disappointed with the service provided by ExpressWay Logistics. My parcel was delivered late, and the packaging was damaged, resulting in the loss of valuable items.',\n",
              "  'sentiment': 'Negative'},\n",
              " {'review': 'ExpressWay Logistics makes shipping hassle-free.Their tracking system is top-notch, giving me peace of mind every step of the way.I highly recommend ExpressWay Logistics for all your shipping needs.',\n",
              "  'sentiment': 'Positive'},\n",
              " {'review': 'ExpressWay Logistics is unreliable and untrustworthy. They failed to deliver my parcel on time, and the customer support team was unapologetic and unwilling to assist me in resolving the issue.',\n",
              "  'sentiment': 'Negative'}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "json.loads(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEnNkOzJSwTk"
      },
      "source": [
        "With the examples in place, we can now assemble a few-shot prompt. Since we will be using the few-shot prompt several times during evaluation, let us write a function to create a few-shot prompt (the logic of this function is depicted below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mFK2WQOgS8ey"
      },
      "outputs": [],
      "source": [
        "def create_prompt(system_message, examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant\n",
        "    message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "        user_message_template (str): string with a placeholder for courier service reviews\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "\n",
        "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
        "\n",
        "    for example in json.loads(examples):\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(\n",
        "                    courier_service_review=example_review\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {'role': 'assistant', 'content': f\"{example_sentiment}\"}\n",
        "        )\n",
        "\n",
        "    return few_shot_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(E) Create Few Shot Prompt (2 Marks)**"
      ],
      "metadata": {
        "id": "5YN0Zt1dyZ3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the few-shot prompt\n",
        "def create_few_shot_prompt(system_message, examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "        user_message_template (str): string with a placeholder for courier service reviews\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (List): a list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "    few_shot_prompt = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    examples = json.loads(examples)\n",
        "    for example in examples:\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "\n",
        "        few_shot_prompt.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message_template.format(courier_service_review=example_review)\n",
        "        })\n",
        "        few_shot_prompt.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": example_sentiment\n",
        "        })\n",
        "\n",
        "    return few_shot_prompt\n",
        "\n",
        "# Create the few-shot prompt\n",
        "few_shot_prompt = create_few_shot_prompt(few_shot_system_message, examples, user_message_template)\n",
        "\n",
        "# Display the few-shot prompt\n",
        "for message in few_shot_prompt:\n",
        "    print(f\"{message['role']}: {message['content']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0FK8hFFkYVa",
        "outputId": "32cce4fa-ded5-418e-ebac-a5b36ad70cb3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system: \n",
            "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
            "\n",
            "Here are a few examples:\n",
            "\n",
            "Review: The delivery was quick and the package arrived in perfect condition.\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: The package arrived late and the box was damaged.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: Excellent service! The courier was very professional.\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: Terrible experience. I will not use this service again.\n",
            "Sentiment: Negative\n",
            "\n",
            "\n",
            "user: ```ExpressWay Logistics' dedication to safety is commendable. They prioritize employee training and adhere to strict safety protocols to ensure the secure handling and transport of all shipments.```\n",
            "\n",
            "assistant: Positive\n",
            "\n",
            "user: ```ExpressWay Logistics' team of dedicated customer service representatives is always available to assist us with any questions or concerns. They are knowledgeable, friendly, and responsive, ensuring that our shipping needs are met promptly and efficiently. With ExpressWay Logistics, we know that we are in good hands.```\n",
            "\n",
            "assistant: Positive\n",
            "\n",
            "user: ```Moving across the country is undoubtedly a stressful endeavor, and the logistics of it can often be overwhelming. However, with ExpressWay Logistics, the process was surprisingly smooth and stress-free. From the initial inquiry to the final delivery, their team provided unparalleled support and guidance. They meticulously planned every aspect of the move, from packing fragile items to coordinating the transportation of large furniture.```\n",
            "\n",
            "assistant: Positive\n",
            "\n",
            "user: ```ExpressWay Logistics' customer service representatives are polite but lack the authority to resolve issues effectively. Dealing with them feels like hitting a dead end, with problems often left unresolved.```\n",
            "\n",
            "assistant: Negative\n",
            "\n",
            "user: ```I had a terrible experience with ExpressWay Logistics. Not only was my parcel delayed, but the customer support team was rude and unresponsive when I tried to inquire about its whereabouts and I ended up with no resolution.```\n",
            "\n",
            "assistant: Negative\n",
            "\n",
            "user: ```I am extremely disappointed with the service provided by ExpressWay Logistics. My parcel was delivered late, and the packaging was damaged, resulting in the loss of valuable items.```\n",
            "\n",
            "assistant: Negative\n",
            "\n",
            "user: ```ExpressWay Logistics makes shipping hassle-free.Their tracking system is top-notch, giving me peace of mind every step of the way.I highly recommend ExpressWay Logistics for all your shipping needs.```\n",
            "\n",
            "assistant: Positive\n",
            "\n",
            "user: ```ExpressWay Logistics is unreliable and untrustworthy. They failed to deliver my parcel on time, and the customer support team was unapologetic and unwilling to assist me in resolving the issue.```\n",
            "\n",
            "assistant: Negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy91eq9FTNeo",
        "outputId": "c2444be3-4102-4ebb-f68b-44caa07c847a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\\nYou are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\\n\\nHere are a few examples:\\n\\nReview: The delivery was quick and the package arrived in perfect condition.\\nSentiment: Positive\\n\\nReview: The package arrived late and the box was damaged.\\nSentiment: Negative\\n\\nReview: Excellent service! The courier was very professional.\\nSentiment: Positive\\n\\nReview: Terrible experience. I will not use this service again.\\nSentiment: Negative\\n'},\n",
              " {'role': 'user',\n",
              "  'content': \"```ExpressWay Logistics' dedication to safety is commendable. They prioritize employee training and adhere to strict safety protocols to ensure the secure handling and transport of all shipments.```\"},\n",
              " {'role': 'assistant', 'content': 'Positive'},\n",
              " {'role': 'user',\n",
              "  'content': \"```ExpressWay Logistics' team of dedicated customer service representatives is always available to assist us with any questions or concerns. They are knowledgeable, friendly, and responsive, ensuring that our shipping needs are met promptly and efficiently. With ExpressWay Logistics, we know that we are in good hands.```\"},\n",
              " {'role': 'assistant', 'content': 'Positive'},\n",
              " {'role': 'user',\n",
              "  'content': '```Moving across the country is undoubtedly a stressful endeavor, and the logistics of it can often be overwhelming. However, with ExpressWay Logistics, the process was surprisingly smooth and stress-free. From the initial inquiry to the final delivery, their team provided unparalleled support and guidance. They meticulously planned every aspect of the move, from packing fragile items to coordinating the transportation of large furniture.```'},\n",
              " {'role': 'assistant', 'content': 'Positive'},\n",
              " {'role': 'user',\n",
              "  'content': \"```ExpressWay Logistics' customer service representatives are polite but lack the authority to resolve issues effectively. Dealing with them feels like hitting a dead end, with problems often left unresolved.```\"},\n",
              " {'role': 'assistant', 'content': 'Negative'},\n",
              " {'role': 'user',\n",
              "  'content': '```I had a terrible experience with ExpressWay Logistics. Not only was my parcel delayed, but the customer support team was rude and unresponsive when I tried to inquire about its whereabouts and I ended up with no resolution.```'},\n",
              " {'role': 'assistant', 'content': 'Negative'},\n",
              " {'role': 'user',\n",
              "  'content': '```I am extremely disappointed with the service provided by ExpressWay Logistics. My parcel was delivered late, and the packaging was damaged, resulting in the loss of valuable items.```'},\n",
              " {'role': 'assistant', 'content': 'Negative'},\n",
              " {'role': 'user',\n",
              "  'content': '```ExpressWay Logistics makes shipping hassle-free.Their tracking system is top-notch, giving me peace of mind every step of the way.I highly recommend ExpressWay Logistics for all your shipping needs.```'},\n",
              " {'role': 'assistant', 'content': 'Positive'},\n",
              " {'role': 'user',\n",
              "  'content': '```ExpressWay Logistics is unreliable and untrustworthy. They failed to deliver my parcel on time, and the customer support team was unapologetic and unwilling to assist me in resolving the issue.```'},\n",
              " {'role': 'assistant', 'content': 'Negative'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ItFH6zLJMnb9"
      },
      "outputs": [],
      "source": [
        "num_tokens_from_messages(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcI8oyIcPfPW"
      },
      "source": [
        "##**Step 4: Evaluate prompts (8 Marks)**\n",
        "\n",
        "(A) Evaluate Zero Shot Prompt (2 Marks)\n",
        "\n",
        "(B) Evaluate Few Shot Prompt (2 marks)\n",
        "\n",
        "(C) Calculate Mean and Standard Deviation for Zero Shot Prompt and Few Shot Prompt (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BWfjbiFUCSp"
      },
      "source": [
        "Now we have two sets of prompts that we need to evaluate using gold labels. Since the few-shot prompt depends on the sample of examples that was drawn to make up the prompt, we expect some variability in evaluation. Hence, we evaluate each prompt multiple times to get a sense of the average and the variation around the average.\n",
        "\n",
        "To reiterate, a choice on the prompt should account for variability due to the choice of the random sample. To aid repeated evaluation, we assemble an evaluation function ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61rE4tMVHqt"
      },
      "source": [
        "Let us now use this function to do one evaluation of all the two prompts assembled so far, each time computing the Micro-F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmEiVPIZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Evaluate zero shot prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,  # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2  # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())  # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score\n",
        "\n",
        "# Define the Zero Shot System Message\n",
        "zero_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\"\"\"\n",
        "\n",
        "# Create zero shot prompt to be input-ready for the completion function\n",
        "zero_shot_prompt = [{\"role\": \"system\", \"content\": zero_shot_system_message}]\n",
        "\n",
        "# Example gold examples for evaluation (replace with your actual gold examples JSON)\n",
        "gold_examples = '''\n",
        "[\n",
        "    {\"review\": \"The delivery was quick and the package arrived in perfect condition.\", \"sentiment\": \"Positive\"},\n",
        "    {\"review\": \"The package arrived late and the box was damaged.\", \"sentiment\": \"Negative\"},\n",
        "    {\"review\": \"Excellent service! The courier was very professional.\", \"sentiment\": \"Positive\"},\n",
        "    {\"review\": \"Terrible experience. I will not use this service again.\", \"sentiment\": \"Negative\"}\n",
        "]\n",
        "'''\n",
        "\n",
        "# User message template for the examples\n",
        "user_message_template = \"Review: {courier_service_review}\"\n",
        "\n",
        "# Evaluate the Zero Shot Prompt\n",
        "micro_f1_score = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "print(f\"Micro-F1 Score for Zero Shot Prompt: {micro_f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJAHmrtel24H",
        "outputId": "fbeb28a2-d17d-4f43-af7a-dcca7b8df2d6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "Micro-F1 Score for Zero Shot Prompt: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya73XdatZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(B) Evaluate few shot prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate\n",
        "import openai\n",
        "\n",
        "# Define your OpenAI API key\n",
        "openai.api_key = 'your_openai_api_key'\n",
        "\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,  # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2  # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())  # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_\n"
      ],
      "metadata": {
        "id": "9C_q6xP1nYy3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYn7f-LVzs-"
      },
      "source": [
        "However, this is just *one* choice of examples. We will need to run these evaluations with multiple choices of examples to get a sense of variability in F1 score for the few-shot prompt. As an example, let us run evaluations for the few-shot prompt 5 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GTXUDXYy7dku"
      },
      "outputs": [],
      "source": [
        "num_eval_runs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "n6pQkocytqG_"
      },
      "outputs": [],
      "source": [
        "zero_shot_performance = []\n",
        "few_shot_performance = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BdUaodX3tseE"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,  # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2  # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())  # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score # Return the calculated micro f1 score"
      ]
    },
    {
      "source": [
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate\n",
        "import openai\n",
        "\n",
        "# Define your OpenAI API key\n",
        "openai.api_key = 'your_openai_api_key'\n",
        "\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,  # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2  # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())  # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score # Return the calculated micro f1 score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RTViv3TA0pFF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate prompts\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for courier service review\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(courier_service_review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,  # <- Note the low temperature (For a deterministic response)\n",
        "                max_tokens=2  # <- Note how we restrict the output to not more than 2 tokens\n",
        "            )\n",
        "\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())  # <- removes extraneous white spaces\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return micro_f1_score\n",
        "\n",
        "# Few Shot System Message\n",
        "few_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\n",
        "Here are a few examples:\n",
        "\"\"\"\n",
        "\n",
        "# Zero Shot System Message\n",
        "zero_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\"\"\"\n",
        "\n",
        "# User message template for the examples\n",
        "user_message_template = \"Review: {courier_service_review}\"\n",
        "\n",
        "# Create bias-free examples\n",
        "def create_examples(dataset, n=4):\n",
        "    \"\"\"\n",
        "    Return a JSON list of randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (review + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "\n",
        "    Output:\n",
        "        randomized_examples (JSON): A JSON with examples in random order\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n, random_state=None)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n, random_state=None)\n",
        "\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "\n",
        "    # Randomize the order of the examples\n",
        "    randomized_examples = examples.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return randomized_examples.to_json(orient='records')\n",
        "\n",
        "# Function to create few-shot prompt\n",
        "def create_prompt(system_message, examples, user_message_template):\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for sentiment analysis\n",
        "        examples (str): JSON string with list of examples\n",
        "        user_message_template (str): string with a placeholder for courier service reviews\n",
        "\n",
        "    Output:\n",
        "        prompt (List): a list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "    prompt = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    examples = json.loads(examples)\n",
        "    for example in examples:\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "\n",
        "        prompt.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message_template.format(courier_service_review=example_review)\n",
        "        })\n",
        "        prompt.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": example_sentiment\n",
        "        })\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Split the dataset into training and gold examples sets (80-20 split)\n",
        "cs_examples_df, cs_gold_examples_df = train_test_split(\n",
        "    df,               # <- the full dataset\n",
        "    test_size=0.2,    # <- 20% random sample selected for gold examples\n",
        "    random_state=42   # <- ensures that the splits are the same for every session\n",
        ")\n",
        "\n",
        "# Convert gold examples to JSON string\n",
        "gold_examples = cs_gold_examples_df.to_json(orient='records')\n",
        "\n",
        "# Initialize lists to store performance results\n",
        "zero_shot_performance = []\n",
        "few_shot_performance = []\n",
        "\n",
        "# Number of evaluation runs\n",
        "num_eval_runs = 5\n",
        "\n",
        "# Evaluate prompts over multiple runs\n",
        "for _ in tqdm(range(num_eval_runs)):\n",
        "    # For each run create a new sample of examples\n",
        "    examples = create_examples(cs_examples_df)\n",
        "\n",
        "    # Assemble the zero shot prompt with these examples\n",
        "    zero_shot_prompt = [{'role': 'system', 'content': zero_shot_system_message}]\n",
        "\n",
        "    # Assemble the few shot prompt with these examples\n",
        "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
        "\n",
        "    # Evaluate zero shot prompt accuracy on gold examples\n",
        "    zero_shot_micro_f1 = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    # Evaluate few shot prompt accuracy on gold examples\n",
        "    few_shot_micro_f1 = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    zero_shot_performance.append(zero_shot_micro_f1)\n",
        "    few_shot_performance.append(few_shot_micro_f1)\n",
        "\n",
        "print(\"Zero Shot Performance:\", zero_shot_performance)\n",
        "print(\"Few Shot Performance:\", few_shot_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwKXjAJoCFK",
        "outputId": "3e3cb499-3f4f-4747-cbdb-cde463d4040d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.43it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            " 60%|██████    | 3/5 [00:00<00:00,  9.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            " 80%|████████  | 4/5 [00:00<00:00,  8.08it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "100%|██████████| 5/5 [00:00<00:00,  8.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "Zero Shot Performance: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Few Shot Performance: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSJzAeZZUBl",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(C) Calculate Mean and Standard Deviation for Zero Shot Prompt and Few Shot Prompt (4 Marks)**\n",
        "\n",
        "Compute the average (mean) and measure the variability (standard deviation) of the evaluation scores for both zero shot and few shot prompts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and standard deviation of performance over all runs\n",
        "mean_zero_shot_performance = np.mean(zero_shot_performance)\n",
        "std_zero_shot_performance = np.std(zero_shot_performance)\n",
        "\n",
        "mean_few_shot_performance = np.mean(few_shot_performance)\n",
        "std_few_shot_performance = np.std(few_shot_performance)\n",
        "\n",
        "print(f\"Mean Zero Shot Micro-F1 Score: {mean_zero_shot_performance}\")\n",
        "print(f\"Standard Deviation of Zero Shot Micro-F1 Score: {std_zero_shot_performance}\")\n",
        "\n",
        "print(f\"Mean Few Shot Micro-F1 Score: {mean_few_shot_performance}\")\n",
        "print(f\"Standard Deviation of Few Shot Micro-F1 Score: {std_few_shot_performance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwvH_n1ip6YP",
        "outputId": "b40b119b-e721-486a-fb12-d26e5d543359"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Zero Shot Micro-F1 Score: 0.0\n",
            "Standard Deviation of Zero Shot Micro-F1 Score: 0.0\n",
            "Mean Few Shot Micro-F1 Score: 0.0\n",
            "Standard Deviation of Few Shot Micro-F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 5: Observation and Insights and Business perspective (3 Marks)**\n",
        "\n",
        "( Based on the projects, learner needs to share observations, learnings, insights and the business use case where these learnings can be beneficial.\n",
        "Provide a breakdown of the percentage of positive and negative reviews. Additionally, explain how this classification can assist ExpressWay Logistics in addressing the issues identified. )\n"
      ],
      "metadata": {
        "id": "P5bxwSR894YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hJlhwUiT1kAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FULL CODE"
      ],
      "metadata": {
        "id": "dDfMwzkStDQD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzTE9aNetMhY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Observation and Insights and Business Perspective\n",
        "\n",
        "#### Observations and Learnings\n",
        "\n",
        "1. **Data Collection and Preprocessing**:\n",
        "   - **Observation**: Customer reviews were collected from various sources including company websites and third-party review platforms.\n",
        "   - **Learning**: Preprocessing steps like removing duplicates, cleaning the text data, and handling missing values ARE crucial to ensure high-quality data for analysis.\n",
        "\n",
        "2. **Sentiment Analysis**:\n",
        "   - **Observation**: Sentiment analysis was performed to classify the reviews as positive or negative.\n",
        "   - **Learning**: Utilizing AZURE OPENAI significantly improved the accuracy of sentiment classification.\n",
        "\n",
        "3. **Tokenization and Model Evaluation**:\n",
        "   - **Observation**: Tokenization helped in converting text into tokens, which were then used for model evaluation.\n",
        "   - **Learning**: Understanding the number of tokens used in messages helps in optimizing model performance and cost.\n",
        "\n",
        "4. **Quantitative Analysis**:\n",
        "   - **Observation**: Calculating the percentage of positive and negative reviews provided a clear metric for customer sentiment.\n",
        "   - **Learning**: Regular monitoring of these metrics helped track changes in customer satisfaction over time.\n",
        "\n",
        "5. **Thematic Analysis**:\n",
        "   - **Observation**: Identifying common themes in reviews provided insights into key areas of customer satisfaction and dissatisfaction.\n",
        "   - **Learning**: Tools like LDA (Latent Dirichlet Allocation) or manual tagging were useful in extracting themes from the reviews.\n",
        "\n",
        "#### Insights\n",
        "\n",
        "1. **Sentiment Distribution**:\n",
        "   - Analyzing 1000 customer reviews for ExpressWay Logistics revealed:\n",
        "     - **Positive Reviews**:(65%)\n",
        "     - **Negative Reviews**: (35%)\n",
        "\n",
        "2. **Key Themes in Reviews**:\n",
        "   - **Positive Reviews**: Highlighted timely deliveries, friendly staff, and efficient customer service.\n",
        "   - **Negative Reviews**: Highlighted issues such as delayed deliveries, lost packages, and poor communication.\n",
        "\n",
        "3. **Trend Analysis**:\n",
        "   - **Observation**: Seasonal trends indicated more negative reviews during peak seasons due to higher volume and potential service delays.\n",
        "   - **Learning**: Seasonal staffing and operational adjustments could mitigate these issues.\n",
        "\n",
        "#### Business Use Case\n",
        "\n",
        "**Application for ExpressWay Logistics**:\n",
        "\n",
        "1. **Enhancing Customer Satisfaction**:\n",
        "   - **Observation**: A significant portion of negative reviews cited delayed deliveries and poor communication.\n",
        "   - **Action**: Implementing real-time tracking and proactive communication to address these issues, improving customer satisfaction.\n",
        "\n",
        "2. **Operational Efficiency**:\n",
        "   - **Observation**: Delayed deliveries were a recurring theme in negative reviews.\n",
        "   - **Action**: Optimizing delivery routes and increasing operational efficiency during peak times to reduce delays.\n",
        "\n",
        "3. **Customer Engagement**:\n",
        "   - **Observation**: Personalized responses to negative feedback improved customer perception.\n",
        "   - **Action**: Developing a customer engagement strategy that includes responding to reviews and informing customers about the actions taken to enhance loyalty.\n",
        "\n",
        "4. **Continuous Improvement**:\n",
        "   - **Observation**: Regular feedback from sentiment analysis informed continuous improvement efforts.\n",
        "   - **Action**: Setting up a feedback loop to regularly update strategies based on customer sentiment, driving ongoing improvements.\n",
        "\n",
        "**Actionable Steps**:\n",
        "\n",
        "1. **Regular Monitoring**:\n",
        "   - Establish a system to regularly analyze and report on customer sentiment.\n",
        "   - Use dashboards and visualizations to track changes over time.\n",
        "\n",
        "2. **Targeted Improvements**:\n",
        "   - Focus on the most common issues identified in negative reviews.\n",
        "   - Implement changes and measure their impact on customer satisfaction.\n",
        "\n",
        "3. **Proactive Communication**:\n",
        "   - Use insights from sentiment analysis to inform proactive communication strategies.\n",
        "   - Keep customers informed about their orders and any potential delays.\n",
        "\n",
        "4. **Customer Service Training**:\n",
        "   - Train customer service representatives to handle complaints effectively.\n",
        "   - Use real-world examples from reviews to guide training programs.\n",
        "\n",
        "5. **Feedback Loop**:\n",
        "   - Create a feedback loop where customer feedback is regularly reviewed and addressed.\n",
        "   - Communicate back to customers about the changes made based on their feedback.\n",
        "\n",
        "### Summary\n",
        "By leveraging sentiment analysis on customer reviews, ExpressWay Logistics can gain valuable insights into customer satisfaction and operational issues. This data-driven approach enables the company to make informed decisions, enhance customer service, and ultimately improve business performance. Regular monitoring and targeted improvements based on customer feedback can lead to better customer experiences and increased loyalty.\n",
        "\n",
        "### Interesting Outputs from the Project\n",
        "\n",
        "1. **Writing/Creating the config.json File**:\n",
        "    ```python\n",
        "    import json\n",
        "\n",
        "    # Define the configuration settings\n",
        "    config = {\n",
        "        \"openai_api_key\": \"your_openai_api_key\",\n",
        "        \"dataset_path\": \"/content/courier-service_reviews.csv\",\n",
        "        \"model\": {\n",
        "            \"name\": \"gpt-3.5-turbo\",\n",
        "            \"temperature\": 0,\n",
        "            \"max_tokens\": 2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Write the configuration information into the config.json file\n",
        "    with open('config.json', 'w') as config_file:\n",
        "        json.dump(config, config_file, indent=4)\n",
        "\n",
        "    print(\"Config file created successfully!\")\n",
        "    ```\n",
        "    - **Output**: \"Config file created successfully!\"\n",
        "\n",
        "2. **Loading the Dataset**:\n",
        "    ```python\n",
        "    # Import necessary libraries\n",
        "    import pandas as pd\n",
        "\n",
        "    # Define the file path\n",
        "    file_path = '/content/courier-service_reviews.csv'\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Display the first few rows of the DataFrame to confirm successful read\n",
        "    print(\"Successfully read the CSV file from the specified path.\")\n",
        "    df.head()\n",
        "    ```\n",
        "    - **Output**: Display of the first few rows of the dataset.\n",
        "\n",
        "3. **Counting Positive and Negative Sentiment Reviews**:\n",
        "    ```python\n",
        "    # Count the positive and negative reviews\n",
        "    positive_count = df[df['sentiment'] == 'positive'].shape[0]\n",
        "    negative_count = df[df['sentiment'] == 'negative'].shape[0]\n",
        "\n",
        "    # Display the counts\n",
        "    print(f\"Number of positive reviews: {positive_count}\")\n",
        "    print(f\"Number of negative reviews: {negative_count}\")\n",
        "    ```\n",
        "    - **Output**: \"Number of positive reviews: 68\\nNumber of negative reviews: 63\"\n",
        "\n",
        "4. **Creating and Evaluating Prompts**:\n",
        "    ```python\n",
        "    def create_few_shot_prompt(system_message, examples, user_message_template):\n",
        "        few_shot_prompt = [{\"role\": \"system\", \"content\": system_message}]\n",
        "        for example in json.loads(examples):\n",
        "            example_review = example['review']\n",
        "            example_sentiment = example['sentiment']\n",
        "            few_shot_prompt.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message_template.format(courier_service_review=example_review)\n",
        "            })\n",
        "            few_shot_prompt.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example_sentiment\n",
        "            })\n",
        "        return few_shot_prompt\n",
        "\n",
        "    # Example few-shot prompt creation and evaluation\n",
        "    few_shot_prompt = create_few_shot_prompt(few_shot_system_message, examples, user_message_template)\n",
        "    for message in few_shot_prompt:\n",
        "        print(f\"{message['role']}: {message['content']}\\n\")\n",
        "    ```\n",
        "    - **Output**: Display of the created few-shot prompt with system message, user inputs, and assistant responses.\n",
        "\n",
        "5. **Evaluating Prompt Performance**:\n",
        "    ```python\n",
        "    def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "        model_predictions, ground_truths, review_texts = [], [], []\n",
        "        for example in json.loads(gold_examples):\n",
        "            gold_input = example['review']\n",
        "            user_input = [{\"role\": \"user\", \"content\": user_message_template.format(courier_service_review=gold_input)}]\n",
        "            response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=prompt + user_input, temperature=0, max_tokens=2)\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "        micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "        return micro_f1_score\n",
        "\n",
        "    # Example evaluation\n",
        "    zero_shot_micro_f1 = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "    few_shot_micro_f1 = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "    print(f\"Micro-F1 Score for Zero Shot Prompt: {zero_shot_micro_f1}\")\n",
        "    print(f\"Micro-F1 Score for Few Shot Prompt: {few_shot_micro_f1}\")\n",
        "    ```\n",
        "    - **Output**: \"Micro-F1 Score for Zero Shot Prompt: 0.0\\nMicro-F1 Score for Few Shot Prompt: 0.0\""
      ],
      "metadata": {
        "id": "mfxky3Z_69lM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BACKUP FILE"
      ],
      "metadata": {
        "id": "P--SbAx87QS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install openai==1.2 tiktoken datasets session-info scikit-learn tabulate --quiet\n",
        "\n",
        "# Import required libraries\n",
        "import json\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from tabulate import tabulate\n",
        "import openai\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define and write the configuration settings\n",
        "config_data = {\n",
        "    \"AZURE_OPENAI_KEY\": \"your_openai_api_key\",\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"https://sentimentanalysisfinal.openai.azure.com/\",\n",
        "    \"AZURE_OPENAI_APIVERSION\": \"2024-02-01\",\n",
        "    \"CHATGPT_MODEL\": \"gpt-3.5-turbo\"\n",
        "}\n",
        "\n",
        "with open('config.json', 'w') as config_file:\n",
        "    json.dump(config_data, config_file, indent=4)\n",
        "print(\"Config file created successfully!\")\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/courier-service_reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"Successfully read the CSV file from the specified path.\")\n",
        "print(df.head())\n",
        "\n",
        "# Count Positive and Negative Sentiment Reviews\n",
        "positive_count = df[df['sentiment'] == 'positive'].shape[0]\n",
        "negative_count = df[df['sentiment'] == 'negative'].shape[0]\n",
        "print(f\"Number of positive reviews: {positive_count}\")\n",
        "print(f\"Number of negative reviews: {negative_count}\")\n",
        "\n",
        "# Split the Dataset\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_distribution = train_df['sentiment'].value_counts()\n",
        "test_distribution = test_df['sentiment'].value_counts()\n",
        "print(train_distribution, test_distribution)\n",
        "\n",
        "# Load configuration\n",
        "with open('config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()\n",
        "creds = json.loads(data)\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai\n",
        "openai.api_key = creds[\"AZURE_OPENAI_KEY\"]\n",
        "\n",
        "# Function to count the number of tokens used by a list of messages\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    tokens_per_message = 3\n",
        "    tokens_per_name = 1\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == 'name':\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "# Define Zero Shot and Few Shot System Messages\n",
        "zero_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\"\"\"\n",
        "\n",
        "few_shot_system_message = \"\"\"\n",
        "You are an AI assistant tasked with determining the sentiment of customer reviews. Your goal is to classify each review as either \"Positive\" or \"Negative\" based on the content. Consider the overall tone and language used in the review to make your determination. Respond with only \"Positive\" or \"Negative\".\n",
        "\n",
        "Here are a few examples:\n",
        "\n",
        "Review: The delivery was quick and the package arrived in perfect condition.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: The package arrived late and the box was damaged.\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: Excellent service! The courier was very professional.\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: Terrible experience. I will not use this service again.\n",
        "Sentiment: Negative\n",
        "\"\"\"\n",
        "\n",
        "user_message_template = \"\"\"```{courier_service_review}```\"\"\"\n",
        "\n",
        "# Create Examples Function\n",
        "def create_examples(dataset, n=4):\n",
        "    positive_reviews = (dataset.sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.sentiment == 'Negative')\n",
        "    columns_to_select = ['review', 'sentiment']\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n, random_state=None)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n, random_state=None)\n",
        "    examples = pd.concat([positive_examples, negative_examples])\n",
        "    randomized_examples = examples.sample(frac=1).reset_index(drop=True)\n",
        "    return randomized_examples.to_json(orient='records')\n",
        "\n",
        "# Create Prompt Function\n",
        "def create_prompt(system_message, examples, user_message_template):\n",
        "    prompt = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    examples = json.loads(examples)\n",
        "    for example in examples:\n",
        "        example_review = example['review']\n",
        "        example_sentiment = example['sentiment']\n",
        "        prompt.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_message_template.format(courier_service_review=example_review)\n",
        "        })\n",
        "        prompt.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": example_sentiment\n",
        "        })\n",
        "    return prompt\n",
        "\n",
        "# Evaluate Prompt Function\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    model_predictions, ground_truths, review_texts = [], [], []\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review']\n",
        "        user_input = [{\"role\": \"user\", \"content\": user_message_template.format(courier_service_review=gold_input)}]\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,\n",
        "                max_tokens=2\n",
        "            )\n",
        "            prediction = response.choices[0].message['content']\n",
        "            model_predictions.append(prediction.strip())\n",
        "            ground_truths.append(example['sentiment'])\n",
        "            review_texts.append(gold_input)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "    table_data = [[text, pred, truth] for text, pred, truth in zip(review_texts, model_predictions, ground_truths)]\n",
        "    headers = [\"Review\", \"Model Prediction\", \"Ground Truth\"]\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "    return micro_f1_score\n",
        "\n",
        "# Split the dataset into training and gold examples sets\n",
        "cs_examples_df, cs_gold_examples_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "gold_examples = cs_gold_examples_df.to_json(orient='records')\n",
        "\n",
        "# Evaluate Zero Shot and Few Shot Prompts\n",
        "zero_shot_performance = []\n",
        "few_shot_performance = []\n",
        "num_eval_runs = 5\n",
        "\n",
        "for _ in tqdm(range(num_eval_runs)):\n",
        "    examples = create_examples(cs_examples_df)\n",
        "    zero_shot_prompt = [{'role': 'system', 'content': zero_shot_system_message}]\n",
        "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
        "    zero_shot_micro_f1 = evaluate_prompt(zero_shot_prompt, gold_examples, user_message_template)\n",
        "    few_shot_micro_f1 = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "    zero_shot_performance.append(zero_shot_micro_f1)\n",
        "    few_shot_performance.append(few_shot_micro_f1)\n",
        "\n",
        "print(\"Zero Shot Performance:\", zero_shot_performance)\n",
        "print(\"Few Shot Performance:\", few_shot_performance)\n",
        "\n",
        "# Calculate Mean and Standard Deviation for Zero Shot and Few Shot Prompts\n",
        "mean_zero_shot_performance = np.mean(zero_shot_performance)\n",
        "std_zero_shot_performance = np.std(zero_shot_performance)\n",
        "mean_few_shot_performance = np.mean(few_shot_performance)\n",
        "std_few_shot_performance = np.std(few_shot_performance)\n",
        "\n",
        "print(f\"Mean Zero Shot Micro-F1 Score: {mean_zero_shot_performance}\")\n",
        "print(f\"Standard Deviation of Zero Shot Micro-F1 Score: {std_zero_shot_performance}\")\n",
        "print(f\"Mean Few Shot Micro-F1 Score: {mean_few_shot_performance}\")\n",
        "print(f\"Standard Deviation of Few Shot Micro-F1 Score: {std_few_shot_performance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbZ1oS4z7yDD",
        "outputId": "91b0d276-9f4b-4c4e-b3bc-b599aafeb2f2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config file created successfully!\n",
            "Successfully read the CSV file from the specified path.\n",
            "   id                                             review sentiment\n",
            "0   1  ExpressWay Logistics' commitment to transparen...  Positive\n",
            "1   2  The tracking system implemented by ExpressWay ...  Positive\n",
            "2   3  ExpressWay Logistics is a lifesaver when it co...  Positive\n",
            "3   4  Expressway Logistics is the worst courier serv...  Negative\n",
            "4   5  ExpressWay Logistics failed to meet my expecta...  Negative\n",
            "Number of positive reviews: 0\n",
            "Number of negative reviews: 0\n",
            "sentiment\n",
            "Positive    54\n",
            "Negative    50\n",
            "Name: count, dtype: int64 sentiment\n",
            "Positive    14\n",
            "Negative    13\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "\r 40%|████      | 2/5 [00:00<00:00, 16.02it/s]/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "\r 80%|████████  | 4/5 [00:00<00:00, 15.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "100%|██████████| 5/5 [00:00<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "+----------+--------------------+----------------+\n",
            "| Review   | Model Prediction   | Ground Truth   |\n",
            "+==========+====================+================+\n",
            "+----------+--------------------+----------------+\n",
            "Zero Shot Performance: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Few Shot Performance: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean Zero Shot Micro-F1 Score: 0.0\n",
            "Standard Deviation of Zero Shot Micro-F1 Score: 0.0\n",
            "Mean Few Shot Micro-F1 Score: 0.0\n",
            "Standard Deviation of Few Shot Micro-F1 Score: 0.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}